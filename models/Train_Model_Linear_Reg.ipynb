{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PickleFilesFolder=\"pickle_files\"\n",
    "_PickleFile_Merged_Listing_NY=\"Merged_Listing_NY\"\n",
    "_PickleFile_Vectorized_Data=\"NY_Vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed( _PickleFile_Merged_Listing_NY + \"_\" + \"c_X_train_Final_1\",X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed( _PickleFile_Merged_Listing_NY + \"_\" + \"c_X_test_Final_1\",X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed( _PickleFile_Merged_Listing_NY + \"_\" + \"c_y_train\",y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed( _PickleFile_Merged_Listing_NY + \"_\" + \"c_y_test\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load( _PickleFile_Merged_Listing_NY + \"_\" + \"c_X_train_Final_1.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.load( _PickleFile_Merged_Listing_NY + \"_\" + \"c_X_test_Final_1.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.load( _PickleFile_Merged_Listing_NY + \"_\" + \"c_y_train.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.load( _PickleFile_Merged_Listing_NY + \"_\" + \"c_y_test.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Cs = [10**i for i in range(-5,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search_CV(X_tr, y_tr, cv_value, optimal_c):\n",
    "    parameters = {'C':optimal_c}\n",
    "    \n",
    "\n",
    "    # Set n_jobs = -1 to use all the processors.\n",
    "    # Increasing the value of cv parameter results in getting more robust value.\n",
    "    print(X_tr.shape, y_tr.shape)\n",
    "    clf = GridSearchCV(LogisticRegression(), parameters, cv=cv_value, scoring='roc_auc', return_train_score=True, n_jobs=-1)\n",
    "    clf.fit(X_tr, y_tr)  \n",
    "    print('Best Penalty:', clf.best_estimator_.get_params()['penalty'])\n",
    "    print('Best C:', clf.best_estimator_.get_params()['C'])\n",
    "\n",
    "    best_C= clf.best_estimator_.get_params()['C']\n",
    "    train_auc = clf.cv_results_['mean_train_score']\n",
    "    train_auc_std = clf.cv_results_['std_train_score']\n",
    "    cv_auc = clf.cv_results_['mean_test_score'] \n",
    "    cv_auc_std= clf.cv_results_['std_test_score']\n",
    "    \n",
    "    \n",
    "    return  clf, best_C, train_auc, cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6082420914751459\n",
      "Best Hyperparameters: {'alpha': 0.01, 'fit_intercept': True, 'normalize': False, 'solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "# grid search linear regression model on the auto insurance dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/auto-insurance.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# define model\n",
    "model = Ridge()\n",
    "# define evaluation\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
    "space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "space['fit_intercept'] = [True, False]\n",
    "space['normalize'] = [True, False]\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='r2', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X, y)\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.01, 'fit_intercept': True, 'normalize': False, 'solver': 'sag'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.01, solver='sag')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, data, iterations = 100):\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        for j in range(iterations):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data[X], \n",
    "                                                                data[Y], \n",
    "                                                                test_size= 0.2)\n",
    "            r2_test.append(metrics.r2_score(y_test,\n",
    "                                            models[i].fit(X_train, \n",
    "                                                         y_train).predict(X_test)))\n",
    "            r2_train.append(metrics.r2_score(y_train, \n",
    "                                             models[i].fit(X_train, \n",
    "                                                          y_train).predict(X_train)))\n",
    "        results[i] = [np.mean(r2_train), np.mean(r2_test)]\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_params = {'alpha':[0.02, 0.024, 0.025, 0.026, 0.03]}\n",
    "ridge_params = {'alpha':[200, 230, 250,265, 270, 275, 290, 300, 500]}\n",
    "\n",
    "models2 = {'OLS': linear_model.LinearRegression(),\n",
    "           'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "           'Ridge': GridSearchCV(linear_model.Ridge(), \n",
    "                               param_grid=ridge_params).fit(df[X], df[Y]).best_estimator_,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print('Best Penalty:', clf.best_estimator_.get_params()['penalty'])\n",
    "    print('Best C:', clf.best_estimator_.get_params()['C'])\n",
    "\n",
    "    best_C= clf.best_estimator_.get_params()['C']\n",
    "    train_auc = clf.cv_results_['mean_train_score']\n",
    "    train_auc_std = clf.cv_results_['std_train_score']\n",
    "    cv_auc = clf.cv_results_['mean_test_score'] \n",
    "    cv_auc_std= clf.cv_results_['std_test_score']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(Cs, auc_tr, auc_cv, plt_title):\n",
    "    import math\n",
    "    log_c =[]\n",
    "\n",
    "    for a in tqdm(Cs):\n",
    "      b = math.log(a)\n",
    "      log_c.append(b)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    plt.plot(log_c, auc_tr, label='AUC_Train')\n",
    "    plt.plot(log_c, auc_cv, label='AUC_Validation')\n",
    "    \n",
    "    plt.scatter(log_c, auc_tr, label='C Train')\n",
    "    plt.scatter(log_c, auc_cv, label='C CV')\n",
    "    \n",
    "    plt.xlabel('Hyperparameter - C')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(\"AUC on various Cs using %s on text features\"%plt_title)\n",
    "    plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_And_Plot_Confusion_Matrix(LOR, x_tr, y_tr, title):\n",
    "  from sklearn.metrics import classification_report \n",
    "  y_pred= LOR.predict(x_tr)\n",
    "  cm= confusion_matrix(y_tr,y_pred)\n",
    "  ax = plt.subplot()\n",
    "  sns.heatmap(cm, annot=True, ax=ax, fmt='g', annot_kws={'size':16})\n",
    "  sns.set(font_scale=1.5) # for label size    \n",
    "  ax.set_xlabel('Predicted Labels')\n",
    "  ax.set_ylabel('Actual Labels')\n",
    "  ax.set_title('Confusion Matrix using %s on text features'%title)\n",
    "  ax.xaxis.set_ticklabels(['0','1'])\n",
    "  ax.yaxis.set_ticklabels(['0','1'])\n",
    "\n",
    "\n",
    "  \n",
    "  plt.grid()\n",
    "  plt.show()\n",
    "\n",
    "  print (\"            \" * 100)\n",
    "  print(\"=\"*50)\n",
    "  \n",
    "  print (\"            \" * 100)\n",
    "  print (\"Classification Report For'Confusion Matrix using %s on text features\"%title)\n",
    "\n",
    "  print (\"            \" * 100)\n",
    "  print(\"=\"*50)\n",
    "\n",
    "  print (\"            \" * 100)\n",
    "  # Refred from https://stackoverflow.com/questions/28200786/how-to-plot-scikit-learn-classification-report\n",
    "  cr= classification_report(y_tr, y_pred,\n",
    "                                   output_dict=True) \n",
    "  sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
